# -*- coding: utf-8 -*-
"""Clark_Whitehead_HW5_Neural_Network_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c1Doh3Fy7Hv0paTgVngGdJeH6vg0vJX1

# Neural Network

In this assignment you will build an artificial neural network _from scratch_, meaning without modern machine learning packages (e.g. scikit-learn, tensorflow, and pytorch). You should use numpy, and can use other standard python libraries if you like. 

## Part 1: Neural Network with Stochastic Gradient Descent (4 points)
Define a class NeuralNetwork that implements an artificial neural network with a single hidden layer. The hidden layer should have non-linear activation function (e.g. Rectificed Linear Units, ReLU), and the output should have a Softmax activation function. Use the template provided. 

The hard part of this is the **train** method, which requires computing lots of gradients. See the [notes](https://laulima.hawaii.edu/access/content/group/MAN.90549.202130/Notes_6__Introduction_to_Neural_Networks.pdf) on Laulima to see the equations for calculating these analytically. Translating these equations into code is non-trivial. The **backpropagation** algorithm is a dynamic programming algorithm that computes the gradients layer by layer, and can be written very elegantly in terms of matrix manipulations (a couple lines of numpy code).

_Reminder: Do NOT copy and paste code from the internet. Write your own code._

## Part 2: Apply Your Model to Fashion Dataset (3 points)
We will test the model on the Fashion MNIST dataset. This is a 10-class classification task designed to be similar to the MNIST digit recognition dataset. The classes are different items of clothing (shoes, shirts, handbags, etc.) instead of digits. Here is an [introduction](https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/) and [github page](https://github.com/zalandoresearch/fashion-mnist).

1. Demonstrate overfitting in your network by plotting the training and test set losses vs. epoch. An *epoch* is one iteration through the training set; in SGD this means one weight update for each training example. You can use a smaller dataset so that you overfit faster, but clearly state how many examples are in your train and test sets.
2. Optimize the hyperparameters (learning rate and number of hidden neurons) of the neural network. Because the test dataset is fairly large (10k examples), you don't need to split off a separate validation set for this analysis. Report the best performance (test accuracy) and the best hyperparameters. 
3. Visualize the 10 test examples with the largest loss.

## Part 3: Better and Faster: Mini-Batch SGD (3 points)
Implement mini-batch gradient descent in your NeuralNetwork train method. This is much more efficient to update the weights on *batches* of training data, e.g. 100 examples at a time, which serves two purposes: (1) each update is a better, less-noisy estimate of the true gradient, and (2) the matrix multiplications can be parallelized for an almost-linear speedup with multiple cores or a GPU (by default, numpy should automatically use multiple CPUs for matrix multiplications). This requires implementing the forward and backpropagation computations efficiently, using matrix multiplications rather than for loops.
"""

import numpy as np
from sklearn.metrics import accuracy_score

# Download Fashion MNIST Dataset
import gzip
import os
from urllib.request import urlretrieve
import numpy as np
import matplotlib.pyplot as plt

def fashion_mnist():
    """
    Download compressed Fashion MNIST data to local directory, and 
    unpack data into numpy arrays. 
    
    Return (train_images, train_labels, test_images, test_labels).

    Args:
        None

    Returns:
        Tuple of (train_images, train_labels, test_images, test_labels), each
            a matrix. Rows are examples. Columns of images are pixel values.
            Columns of labels are a onehot encoding of the correct class.
    """
    url = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'
    files = ['train-images-idx3-ubyte.gz',
             'train-labels-idx1-ubyte.gz',
             't10k-images-idx3-ubyte.gz',
             't10k-labels-idx1-ubyte.gz']
    path = './' # Download data to current directory.
    os.makedirs(path, exist_ok=True) # Create path if it doesn't exist.

    # Download any missing files
    for file in files:
        if file not in os.listdir(path):
            urlretrieve(url + file, os.path.join(path, file))
            print("Downloaded %s to %s" % (file, path))

    def _images(path):
        """Return images loaded locally."""
        with gzip.open(path) as f:
            # First 16 bytes are magic_number, n_imgs, n_rows, n_cols
            pixels = np.frombuffer(f.read(), 'B', offset=16)
        return pixels.reshape(-1, 784).astype('float32') / 255

    def _labels(path):
        """Return labels loaded locally."""
        with gzip.open(path) as f:
            # First 8 bytes are magic_number, n_labels
            integer_labels = np.frombuffer(f.read(), 'B', offset=8)

        def _onehot(integer_labels):
            """Return matrix whose rows are onehot encodings of integers."""
            n_rows = len(integer_labels)
            n_cols = integer_labels.max() + 1
            onehot = np.zeros((n_rows, n_cols), dtype='uint8')
            onehot[np.arange(n_rows), integer_labels] = 1
            return onehot

        return _onehot(integer_labels)

    train_images = _images(os.path.join(path, files[0]))
    train_labels = _labels(os.path.join(path, files[1]))
    test_images = _images(os.path.join(path, files[2]))
    test_labels = _labels(os.path.join(path, files[3]))
    
    return train_images, train_labels, test_images, test_labels

train_images, train_labels, test_images, test_labels = fashion_mnist()

# Plot examples from dataset.
plt.figure(1, figsize=(14,6))
for i in range(10):
    plt.subplot(1,10, i+1)
    plt.imshow(train_images[i,:].reshape(28,28), cmap='bone')
    plt.title(f'Label: {train_labels[i].argmax()}')
    plt.xticks([])
    plt.yticks([])

print(test_images.shape)

train_images_1000 = train_images[0:1000, :]
train_labels_1000 = train_labels[0:1000, :]

train_images_100 = train_images[0:100, :]
train_labels_100 = train_labels[0:100, :]

test_images_1000 = test_images[0:1000, :]
test_labels_1000 = test_labels[0:1000, :]

test_images_100 = test_images[0:100, :]
test_labels_100 = test_labels[0:100, :]

train_images_10 = train_images[0:10, :]
train_labels_10 = train_labels[0:10, :]

train_images_1 = train_images[0:1, :]
train_labels_1 = train_labels[0:1, :]

print(train_images_10.shape)

#x_train shape = 11x4
x_train = np.array([[1.2, 4.3, 2.1, 1.9],
                    [6.2, 8.3, 5.1, 9.9],
                    [2.3, 4.3, 3.1, 0.9],
                    [4.1, 4.4, 1.1, 0.3],
                    [6.1, 7.1, 8.1, 9.1],
                    [1.0, 2.0, 1.0, 1.0],
                    [5.1, 5.1, 5.1, 5.1],
                    [1.8, 4.0, 3.9, 2.7],
                    [4.4, 0.8, 1.9, 2.7],
                    [6.9, 8.8, 5.7, 7.1]])

#y_train shape = 1x11
y_train = np.array([[1,0],
                    [0,1],
                    [1,0],
                    [1,0],
                    [0,1],
                    [1,0],
                    [0,1],
                    [1,0],
                    [1,0],
                    [0,1]])

# Part 1: Defining the neural network.
class NeuralNetwork():
    
    def __init__(self, inputs, hidden, outputs):
        """
        Initialize a simple neural network with a single hidden layer.
        This method randomly initializes the parameters of the model,
        saving them as private variables.
        
        Each layer is parameterized by a weight matrix and a bias vector; 
        a useful trick is store the weights and biases for a layer together,
        as a single matrix.
        
        Args:
            inputs: int, input dimension
            hidden: int, number of hidden neurons
            outputs: int, number of output neurons
        Returns:
            None
        """
        
        # Initialize the weights and biases of the neural network as private variables.
        # Store a weight matrix for each layer.

        self.hidden = hidden

        self.train_acc = [] 
        self.test_acc = []
        self.train_loss = []
        self.test_loss = []
        
        self.w1 = 2 * np.random.rand(inputs, hidden) - 1
        self.w2 = 2 * np.random.rand(hidden, outputs) - 1 


    def loss(self, y_true, y_pred):
        """
        Compute categorical cross-entropy loss function. 
        
        Sum loss contributions over the outputs (axis=1), but 
        average over the examples (axis=0)
        
        Args: 
            y_true: NxD numpy array with N examples and D outputs (one-hot labels).
            y_pred: NxD numpy array with N examples and D outputs (probabilities).
        Returns:
            loss: array of length N representing loss for each example.
        """

        # WRITE ME

        loss = np.sum(y_pred * y_true, axis=1) #multiply the one_hot from y_true by it's predicted value
        loss = -np.log(loss) #take neg log of the predicted value
        loss = np.mean(loss) #take mean of loss from whole batch
        return loss

    def softmax(self, input):
      normalization = input - np.max(input, axis=1, keepdims=True) #subtract max value from all
                                                                   #values in order to keep exp from exploding
      exp = np.exp(input) #take e to the x where x=inputs for all z before softmax
      output = exp / np.sum(exp, axis=1, keepdims=True) #divide all exp by sum so their combined total = 1
      output = np.clip(output, 1e-7, 1 - 1e-7) #clip in case there is a 0 or a 1. Necessary for loss so we don't take log of 0
      # output = np.sum(output * targets, axis=1, keepdims=1)
      return output

    def relu(self, input):
      output = np.maximum(0, input)
      return output
        
    def predict(self, X):
        """
        Make predictions on inputs X.
        Args:
            X: NxM numpy array where n-th row is an input.
        Returns: 
            y_pred: NxD array where n-th row is vector of probabilities.
        """
        # WRITE ME

        self.h = np.dot(X, self.w1)
        self.h = self.relu(self.h)
        output = np.dot(self.h, self.w2)
        y_pred = self.softmax(output)
        return y_pred 

    def evaluate(self, X, y):
        """
        Make predictions and compute loss.
        Args:
            X: NxM numpy array where n-th row is an input.
            y: NxD numpy array with N examples and D outputs (one-hot labels).
        Returns:
            loss: array of length N representing loss for each example.
        """
        # WRITE ME
        
        y_pred = self.predict(X)
        y_true = y

        loss = self.loss(y_pred, y_true)

        return loss

    def train(self, X, y, X_test, y_test, max_epochs, lr=0.0001):
        """
        Train the neural network using stochastic gradient descent.
        
        Args:
            X: NxM numpy array where n-th row is an input.
            y: NxD numpy array with N examples and D outputs (one-hot labels).
            lr: scalar learning rate. Use small value for debugging.
            max_epochs: int, each epoch is one iteration through the training data.
        Returns:
            None
        """
        # WRITE ME


        for i in range(max_epochs):

          y_pred = self.predict(X)
          self.train_loss.append(self.loss(y, y_pred))

          acc_pred = np.where(y_pred > 0.5, 1, 0)
          self.train_acc.append(accuracy_score(acc_pred, y))

          #derivatives
          dl_dz = y_pred - y 
          dl_dw2 = np.dot(self.h.T, dl_dz) 
          dh_dzh = self.h > 0 
          dl_dzh = np.dot(dl_dz, self.w2.T) * dh_dzh 
          dl_dw1 = np.dot(X.T, dl_dzh) 

          #updates
          self.w1 -= (lr*dl_dw1)
          self.w2 -= (lr*dl_dw2)
        
          y_test_pred = self.predict(X_test)
          self.test_loss.append(self.loss(y_test, y_test_pred))

          acc_pred = np.where(y_test_pred > 0.5, 1, 0)
          self.test_acc.append(accuracy_score(acc_pred, y_test))

#Part 2: Apply Your Model to Fashion Dataset
nn1 = NeuralNetwork(784, 20, 10)

#train on 60k train samples @ 1000 samples per batch. 60 batches
for i in range(60):
  j = i + 1
  i = i * 1000
  j = j * 1000
  nn1.train(train_images[i:j], train_labels[i:j], test_images, test_labels, 50)

"""# Images with most loss"""

y_pred = nn1.predict(test_images)
error = y_pred - test_labels
error = np.sum(error, axis=1)
arg = error.argsort()
top = arg[0:10]
print(top)
test_images_top = test_images[top]
# Plot examples from dataset.
plt.figure(1, figsize=(14,6))
for i in range(10):
    plt.subplot(1,10, i+1)
    plt.imshow(test_images_top[i].reshape(28,28), cmap='bone')
    plt.title(f'Label: {train_labels[i].argmax()}')
    plt.xticks([])
    plt.yticks([])

"""# Ignore nn2. It's my own test data"""

nn2 = NeuralNetwork(4, 20, 2)
nn2.train(x_train, y_train, x_train, y_train, 1000) #my own play test dataset - requires min 1000 epochs to work

nn2_loss = nn2.evaluate(x_train, y_train)

print(nn2_loss)
print(np.round(nn2.predict(x_train), 2))
print(y_train)

print("Final train loss = ", nn1.train_loss[-1])

import matplotlib.pyplot as plt
plt.plot(nn1.train_loss, color='blue', label='Train losses')
plt.plot(nn1.test_loss, color='red', label='Test losses')
plt.legend()
plt.title('Cross Entropy Loss vs Epochs')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.show()

print("Final train accuracy = ", nn1.train_acc[-1])

import matplotlib.pyplot as plt
plt.plot(nn1.train_acc, color='blue', label='Train Acc')
plt.plot(nn1.test_acc, color='red', label='Test losses')
plt.legend()
plt.title('Accuracy vs Epochs')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.show()

#Part 3: Better and Faster: Mini-Batch SGD
class NeuralNetworkBatch():
    
    def __init__(self, inputs, hidden, outputs):
        """
        Initialize a simple neural network with a single hidden layer.
        This method randomly initializes the parameters of the model,
        saving them as private variables.
        
        Each layer is parameterized by a weight matrix and a bias vector; 
        a useful trick is store the weights and biases for a layer together,
        as a single matrix.
        
        Args:
            inputs: int, input dimension
            hidden: int, number of hidden neurons
            outputs: int, number of output neurons
        Returns:
            None
        """
        
        # Initialize the weights and biases of the neural network as private variables.
        # Store a weight matrix for each layer.

        self.hidden = hidden

        self.train_acc = [] 
        self.test_acc = []
        self.train_loss = []
        self.test_loss = []
        
        self.w1 = 2 * np.random.rand(inputs, hidden) - 1    
        self.w2 = 2 * np.random.rand(hidden, outputs) - 1 


    def loss(self, y_true, y_pred):
        """
        Compute categorical cross-entropy loss function. 
        
        Sum loss contributions over the outputs (axis=1), but 
        average over the examples (axis=0)
        
        Args: 
            y_true: NxD numpy array with N examples and D outputs (one-hot labels).
            y_pred: NxD numpy array with N examples and D outputs (probabilities).
        Returns:
            loss: array of length N representing loss for each example.
        """

        # WRITE ME

        loss = np.sum(y_pred * y_true, axis=1) #multiply the one_hot from y_true by it's predicted value
        loss = -np.log(loss) #take neg log of the predicted value
        loss = np.mean(loss) #take mean of loss from whole batch
        return loss

    def softmax(self, input):
      normalization = input - np.max(input, axis=1, keepdims=True) #subtract max value from all
                                                                   #values in order to keep exp from exploding
      exp = np.exp(input) #take e to the x where x=inputs for all z before softmax
      output = exp / np.sum(exp, axis=1, keepdims=True) #divide all exp by sum so their combined total = 1
      output = np.clip(output, 1e-7, 1 - 1e-7) #clip in case there is a 0 or a 1. Necessary for loss so we don't take log of 0
      # output = np.sum(output * targets, axis=1, keepdims=1)
      return output

    def relu(self, input):
      output = np.maximum(0, input)
      return output
        
    def predict(self, X):
        """
        Make predictions on inputs X.
        Args:
            X: NxM numpy array where n-th row is an input.
        Returns: 
            y_pred: NxD array where n-th row is vector of probabilities.
        """
        # WRITE ME

        self.h = np.dot(X, self.w1)
        self.h = self.relu(self.h)
        output = np.dot(self.h, self.w2)
        y_pred = self.softmax(output)
        return y_pred 

    def evaluate(self, X, y):
        """
        Make predictions and compute loss.
        Args:
            X: NxM numpy array where n-th row is an input.
            y: NxD numpy array with N examples and D outputs (one-hot labels).
        Returns:
            loss: array of length N representing loss for each example.
        """
        # WRITE ME
        
        y_pred = self.predict(X)
        y_true = y

        loss = self.loss(y_pred, y_true)

        return loss

    def train(self, X, y, X_test, y_test, max_epochs, lr=0.0001, batch_size=100):
        """
        Train the neural network using stochastic gradient descent.
        
        Args:
            X: NxM numpy array where n-th row is an input.
            y: NxD numpy array with N examples and D outputs (one-hot labels).
            lr: scalar learning rate. Use small value for debugging.
            max_epochs: int, each epoch is one iteration through the training data.
        Returns:
            None
        """
        # WRITE ME

        

        for i in range(max_epochs):

          for j in range(len(X)/ batch_size):

            start = j * batch_size
            stop = j * len(X) / batch_size

            y_pred = self.predict(X[start:stop])
            self.train_loss.append(self.loss(y[start:stop], y_pred[start:stop]))

            acc_pred = np.where(y_pred[start:stop] > 0.5, 1, 0)
            self.train_acc.append(accuracy_score(acc_pred[start:stop], y))

            #derivatives
            dl_dz = y_pred[start:stop] - y[start:stop] 
            dl_dw2 = np.dot(self.h.T, dl_dz) 
            dh_dzh = self.h > 0 
            dl_dzh = np.dot(dl_dz, self.w2.T) * dh_dzh 
            dl_dw1 = np.dot(X[start:stop].T, dl_dzh) 

            #updates
            self.w1 -= (lr*dl_dw1)
            self.w2 -= (lr*dl_dw2)
          
            y_test_pred = self.predict(X_test)
            self.test_loss.append(self.loss(y_test, y_test_pred))

            acc_pred = np.where(y_test_pred > 0.5, 1, 0)
            self.test_acc.append(accuracy_score(acc_pred, y_test))